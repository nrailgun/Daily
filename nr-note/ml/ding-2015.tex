\input{head}

\title{Deep feature learning with relative distance comparison for person re-identification}
\maketitle

\section{Introduction}
Person re-identification（简称 pid）的难处在于在外形、结构巨大变化的情况下，既要找到个体差异，还要找到个体相似之处。
文章提供了一种\textbf{可规模化距离驱动特征学习框架}。具体而言，给定标记 ID 的人像，先产生 triplet units
$ \left< O_1, O_{match}, O_{mismatch} \right>$，输入卷积神经网络产生特征，特征输入 L2 matric 产生距离。
对于 triplet 数量巨大的问题，本文提出一种 triplet generation scheme 并改进梯度下降算法，使得计算代价主要依赖于图片
数量而不是 triplet 数量。

\section{Model}
Let $O_i = \left\{ O_i^1, O_i^2, O_i^3 \right\}$ denote triplet. Let $W = \left\{ W_j \right\}$ denote network
parameter, and $F_W(I)$ denote network output with respect image $I$. For a training triplet $O_i$, $F_W(I)$
should satisfy such condition:
\[
	\parallel F_W(O^1) - F_W(O^2) \parallel < \parallel F_W(O^1) - F_W(O^3) \parallel
\]

Define objective:
\[
	f(W, O) = \sum_{i=1}^{n} \max \left\{
		\parallel F_W(O_i^1) - F_W(O_i^2) \parallel^2 - \parallel F_W(O_i^1) - F_W(O_i^3) \parallel^2,
		C \right\}
\]
In this paper, C is set $-1$.

\section{Learning}

\subsection{Triplet based}
We introduce $d(W, O_i)$, which denotes the difference in distance between matched / mismatched pairs:
\[
	d(W, O_i) = \parallel F_W(O_i^1) - F_W(O_i^2) \parallel^2 - \parallel F_W(O_i^1) - F_W(O_i^3) \parallel^2
\]
Rewrite objective as:
\[
	f(W, O_i) = \sum_{O_i} \max \{ d(W, O_i), C \}
\]

Then, the partial derivative of objective becomes:
\[
	\frac{\delta f}{\delta W_j} = \sum_{O_i} h(O_i)
\]
\[
	h(O_i) = 
	\begin{cases}
		\frac{ \delta d(W, O_i) }{ \delta W_j } & f(W, O_i) > C \\
		0										& f(W, O_i) \leqq C
	\end{cases}
\]

By the definition of $d(W, O_i)$, we can obtain the gradient of it as follows:
\[
\begin{split}
	\frac{ \delta d(W, O_i) }{ \delta W_j } =
		2 (F_W(O_i^1) - F_W(O_i^2)) \cdot \frac{ \delta F_W(O_i^1) - \delta F_W(O_i^2) }{ \delta W_j }
		\\ -
		2 (F_W(O_i^1) - F_W(O_i^3)) \cdot \frac{ \delta F_W(O_i^1) - \delta F_W(O_i^3) }{ \delta W_j }
\end{split}
\]
We can calculate gradient easily given $\frac{\delta F_W(O_i^1)}{\delta W_j}$, $\frac{\delta F_W(O_i^2)}{\delta W_j}$,
and $\frac{\delta F_W(O_i^3)}{\delta W_j}$.

\subsection{Image based gradient descent}
In the triplet based gradient descent algorithm, the number of network propagation depends on the number
of triplets. But if one image appears in different triplets, the propagation result can be reused.

We introduce $f$, which denotes conventional CNN objective:
\[
	f(I_1, I_2, \dots, I_n) = \frac{1}{n} \sum_{i=1}^{n} loss(F_W(I_i))
\]
\[
	\frac{\delta f}{W} = \frac{1}{n} \sum_{i=1}^{n} \frac{\delta loss(F_W(I_i))}{\delta W}
\]

For network weight $W^l$ and image $I_i$, the gradient can be calculated by the chain rule, which is given
as follows:
\[
	\frac{\delta loss(F_W(I_i))}{\delta W^l} = \frac{\delta loss(F_W(I_i))}{\delta X_i^l}
		\frac{\delta X_i^l}{\delta W^l}
\]
\[
	\frac{\delta loss(F_W(I_i))}{\delta X_i^l} = \frac{\delta loss(F_W(I_i))}{ \delta X_i^{l+1} }
		\frac{ \delta X_i^{l+1} }{ \delta X_i^l }
\]

The diffculty lies on the impossibility of writing the objective function directly as the sum of the loss
functions on the images, for taking this form:
\[
	f = \sum_{i=1}{n} loss(F_W(O_i^1), F_W(O_i^2), F_W(O_i^3))
\]
This objective function can be seen as follows, where $\{I'_k\}$ represents the set of all the distinct
images in the triplets, i.e. $\{I'_k\} = \{O_i^1\} \bigcup \{O_i^3\} \bigcup \{O_i^3\}$ and $m$ is the
number of images:
\[
	f = f(F_W(I'_1), F_W(I'_2), F_W(I'_3), \dots, F_W(I'_m))
\]
As $F_W(I'_k)$ is some function of feature map $X_k^l$ at the $l$-th layer, the objective function can also
be seen as follows:
\[
	f = f(X_1^l, X_2^l, X_3^l, \dots, X_m^l)
\]
Then the derivative rule give us the following equations:
\[
	\frac{\delta f}{\delta W^l} = \sum_{k=1}^{m} \frac{\delta f}{\delta X_k^l} \frac{\delta X_k^l}{\delta W^l}
\]
\[
	\frac{\delta f}{\delta X_k^l} = \frac{\delta f}{\delta X_k^{l+1}} \frac{\delta X_k^{l+1}}{\delta X_k^l}
\]
And the derivative with respect to the output of each image can be obtained as follows:
$$
	\frac{\delta f}{\delta F_W(I'_k)} = \sum_{i=1}^{n} \frac
		{\delta \max \left\{ \parallel F_W(O_i^1) - F_W(O_i^2) \parallel^2
			- \parallel F_W(O_i^1) - F_W(O_i^3) \parallel^2, C \right\} }
		{\delta F_W(I'_K)}
$$

\section{Batch learning and triplet generation}
Suppose we have a labelled dataset with $M$ classes (persons) with $N$ images each. The number of possible
triplet would be $M(M-1)N^2(N-1)$. It's necessary to train the network iteratively. In each iteration,
only a small part of triplets is selected. In each iteration, we select a fixed number of classes, and
for each image in classes, we randomly construct a large number of triplets in which the matched references
are randomly selected from the same class and the mismatched references are randomly selected the remaining
classes.

\section{Conclusion}
I guess this paper proposes a approach to compute gradient without 3 propagations, by using chain rule.

\input{foot}